<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>04 - ML Services & Inference | Cresta AI Platform Technical Assessment</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg-primary: #0f172a; --bg-secondary: #1e293b; --bg-tertiary: #334155;
            --text-primary: #f1f5f9; --text-secondary: #94a3b8; --text-muted: #64748b;
            --accent-blue: #3b82f6; --accent-cyan: #22d3ee; --accent-green: #22c55e;
            --accent-yellow: #eab308; --accent-orange: #f97316; --accent-red: #ef4444;
            --accent-purple: #a855f7; --border-color: #334155;
        }
        * { box-sizing: border-box; margin: 0; padding: 0; }
        body { font-family: 'Inter', sans-serif; background: var(--bg-primary); color: var(--text-primary); line-height: 1.7; font-size: 15px; }
        .container { max-width: 1400px; margin: 0 auto; padding: 40px 60px; }
        header { background: linear-gradient(135deg, var(--bg-secondary) 0%, var(--bg-tertiary) 100%); border-bottom: 1px solid var(--border-color); padding: 48px 0; }
        header .container { display: flex; justify-content: space-between; align-items: flex-start; }
        .header-content h1 { font-size: 2.25rem; font-weight: 700; margin-bottom: 8px; background: linear-gradient(135deg, var(--text-primary) 0%, var(--accent-purple) 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent; }
        .header-content .subtitle { font-size: 1.125rem; color: var(--text-secondary); }
        .doc-badge { font-family: 'JetBrains Mono', monospace; background: linear-gradient(135deg, var(--accent-purple) 0%, var(--accent-blue) 100%); padding: 8px 16px; border-radius: 8px; font-size: 0.875rem; font-weight: 600; }
        nav.doc-nav { background: var(--bg-secondary); border-bottom: 1px solid var(--border-color); padding: 12px 0; position: sticky; top: 0; z-index: 100; }
        nav.doc-nav ul { list-style: none; display: flex; gap: 6px; max-width: 1400px; margin: 0 auto; padding: 0 60px; overflow-x: auto; }
        nav.doc-nav a { color: var(--text-secondary); text-decoration: none; padding: 10px 16px; border-radius: 8px; font-size: 0.875rem; font-weight: 500; white-space: nowrap; transition: all 0.2s; }
        nav.doc-nav a:hover { background: var(--bg-tertiary); color: var(--text-primary); }
        nav.doc-nav a.active { background: linear-gradient(135deg, var(--accent-purple) 0%, var(--accent-blue) 100%); color: white; }
        section { margin-bottom: 48px; }
        h2 { font-size: 1.5rem; font-weight: 700; margin: 48px 0 24px; padding-bottom: 12px; border-bottom: 2px solid var(--border-color); display: flex; align-items: center; gap: 12px; }
        h2::before { content: ''; display: inline-block; width: 4px; height: 24px; background: linear-gradient(180deg, var(--accent-purple) 0%, var(--accent-blue) 100%); border-radius: 2px; }
        h3 { font-size: 1.125rem; font-weight: 600; margin: 32px 0 16px; color: var(--accent-cyan); }
        p { margin-bottom: 16px; color: var(--text-secondary); }
        .legend { display: flex; flex-wrap: wrap; gap: 12px; padding: 20px 24px; background: var(--bg-secondary); border: 1px solid var(--border-color); border-radius: 12px; margin-bottom: 32px; }
        .legend-item { display: flex; align-items: center; gap: 8px; font-size: 0.875rem; padding: 6px 12px; background: var(--bg-tertiary); border-radius: 6px; }
        .context-box { background: linear-gradient(135deg, rgba(168, 85, 247, 0.08) 0%, rgba(59, 130, 246, 0.08) 100%); border: 1px solid var(--border-color); border-left: 4px solid var(--accent-purple); border-radius: 0 12px 12px 0; padding: 24px; margin: 24px 0; }
        .context-box strong { color: var(--accent-purple); }
        .diagram-container { background: var(--bg-secondary); border: 1px solid var(--border-color); border-radius: 12px; padding: 32px; margin: 24px 0; overflow-x: auto; }
        .diagram-title { font-size: 1rem; font-weight: 600; margin-bottom: 24px; color: var(--text-primary); display: flex; align-items: center; gap: 10px; }
        .diagram-title::before { content: ''; display: inline-block; width: 4px; height: 20px; background: var(--accent-purple); border-radius: 2px; }
        table { width: 100%; border-collapse: collapse; margin: 24px 0; font-size: 0.925rem; background: var(--bg-secondary); border-radius: 12px; overflow: hidden; border: 1px solid var(--border-color); }
        th, td { padding: 14px 18px; text-align: left; border-bottom: 1px solid var(--border-color); }
        th { background: var(--bg-tertiary); font-weight: 600; text-transform: uppercase; font-size: 0.75rem; letter-spacing: 0.05em; }
        tr:last-child td { border-bottom: none; }
        tr:hover td { background: rgba(168, 85, 247, 0.05); }
        .badge { display: inline-flex; align-items: center; gap: 6px; padding: 4px 10px; border-radius: 6px; font-size: 0.75rem; font-weight: 600; }
        .badge.confirmed { background: rgba(34, 197, 94, 0.15); color: var(--accent-green); }
        .badge.pending { background: rgba(234, 179, 8, 0.15); color: var(--accent-yellow); }
        .badge.high { background: rgba(239, 68, 68, 0.15); color: var(--accent-red); }
        .badge.medium { background: rgba(249, 115, 22, 0.15); color: var(--accent-orange); }
        .badge.low { background: rgba(34, 197, 94, 0.15); color: var(--accent-green); }
        .badge.ocean1 { background: rgba(168, 85, 247, 0.15); color: var(--accent-purple); }
        .badge.fireworks { background: rgba(249, 115, 22, 0.15); color: var(--accent-orange); }
        .card-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(320px, 1fr)); gap: 20px; margin: 24px 0; }
        .card { background: var(--bg-secondary); border: 1px solid var(--border-color); border-radius: 12px; padding: 24px; }
        .card h4 { font-size: 1rem; font-weight: 600; margin-bottom: 16px; }
        .card.latency { border-left: 4px solid var(--accent-yellow); }
        .card.cost { border-left: 4px solid var(--accent-green); }
        .card.operational { border-left: 4px solid var(--accent-purple); }
        .highlight-box { background: var(--bg-secondary); border: 1px solid var(--border-color); border-radius: 12px; padding: 24px; margin: 24px 0; }
        .highlight-box.info { border-left: 4px solid var(--accent-blue); }
        .highlight-box.danger { border-left: 4px solid var(--accent-red); }
        .highlight-box.success { border-left: 4px solid var(--accent-green); }
        .two-col { display: grid; grid-template-columns: 1fr 1fr; gap: 24px; }
        code { font-family: 'JetBrains Mono', monospace; background: var(--bg-tertiary); padding: 2px 8px; border-radius: 4px; font-size: 0.875rem; color: var(--accent-cyan); }
        .code-block { background: var(--bg-tertiary); border: 1px solid var(--border-color); border-radius: 8px; padding: 20px; margin: 16px 0; overflow-x: auto; }
        .code-block pre { margin: 0; color: var(--text-primary); font-family: 'JetBrains Mono', monospace; font-size: 0.875rem; line-height: 1.6; }
        footer { background: var(--bg-secondary); border-top: 1px solid var(--border-color); padding: 32px 0; margin-top: 64px; }
        footer .container { display: flex; justify-content: space-between; font-size: 0.875rem; color: var(--text-muted); }
        @media (max-width: 1024px) { .container { padding: 24px; } nav.doc-nav ul { padding: 0 24px; } .two-col { grid-template-columns: 1fr; } }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <div class="header-content">
                <h1>ML Services & Inference</h1>
                <p class="subtitle">Ocean-1 Foundation Model, Fireworks AI Partnership, and Real-time Inference</p>
            </div>
            <span class="doc-badge">Document 04</span>
        </div>
    </header>

    <nav class="doc-nav">
        <ul>
            <li><a href="00-executive-summary.html">00 Summary</a></li>
            <li><a href="01-overall-architecture.html">01 Architecture</a></li>
            <li><a href="02-amazon-connect-integration.html">02 Connect</a></li>
            <li><a href="03-voice-stack-architecture.html">03 Voice Stack</a></li>
            <li><a href="#" class="active">04 ML Services</a></li>
            <li><a href="05-realtime-dataflow-sequences.html">05 Data Flow</a></li>
            <li><a href="06-data-security-architecture.html">06 Security</a></li>
        </ul>
    </nav>

    <div class="container">
        <div class="legend">
            <div class="legend-item">‚è±Ô∏è Latency Risk</div>
            <div class="legend-item">üí∞ Cost Risk</div>
            <div class="legend-item">‚öôÔ∏è Operational Risk</div>
            <div class="legend-item">üß† Model Risk</div>
            <div class="legend-item">üü° Requires Follow-up</div>
            <div class="legend-item">‚úÖ Confirmed</div>
        </div>

        <section>
            <h2>ML Services Architecture Overview</h2>
            
            <div class="context-box">
                <strong>Context:</strong> Cresta's ML stack centers on Ocean-1, their proprietary foundation model for contact center interactions, hosted by Fireworks AI for inference. The <code>orchestrator</code> service receives transcript events, constructs prompts with conversation history and knowledge articles, calls Ocean-1 for inference, and publishes suggestions to agents via <code>clientsubscription</code>. A separate <code>knowledgeretrieval</code> service uses RAG (Retrieval-Augmented Generation) to find relevant knowledge base articles.
            </div>

            <div class="diagram-container">
                <div class="diagram-title">ML Inference Pipeline</div>
                <div class="mermaid">
flowchart TB
    subgraph TranscriptSource["Transcript Source"]
        Redis["Redis<br/>Event Stream"]
    end

    subgraph Orchestrator["orchestrator Service"]
        EventConsumer["Event<br/>Consumer"]
        PromptBuilder["Prompt<br/>Builder"]
        HistoryManager["Conversation<br/>History Manager"]
        KnowledgeCaller["Knowledge<br/>Retrieval Caller"]
    end

    subgraph KnowledgeRAG["knowledgeretrieval Service"]
        EmbeddingGen["Embedding<br/>Generation"]
        VectorSearch["Vector<br/>Search"]
        Reranker["Result<br/>Reranker"]
        Pinecone[("Pinecone<br/>Vector DB")]
    end

    subgraph MLInference["ML Inference"]
        FireworksAPI["Fireworks AI<br/>API ‚è±Ô∏è"]
        Ocean1["Ocean-1<br/>Foundation Model üß†"]
        ResponseParser["Response<br/>Parser"]
    end

    subgraph AgentDelivery["Agent Delivery"]
        ClientSub["clientsubscription"]
        AgentApp["Cresta Agent<br/>App"]
    end

    subgraph DataStores["Supporting Data"]
        PostgresConv[("PostgreSQL<br/>Conversations")]
        PostgresKB[("PostgreSQL<br/>Knowledge Base")]
        S3Cache[("S3<br/>Model Cache")]
    end

    Redis --> EventConsumer --> PromptBuilder
    PromptBuilder --> HistoryManager --> PostgresConv
    PromptBuilder --> KnowledgeCaller --> EmbeddingGen
    EmbeddingGen --> VectorSearch --> Pinecone
    VectorSearch --> Reranker --> PromptBuilder
    PostgresKB -.-> VectorSearch

    PromptBuilder --> FireworksAPI --> Ocean1
    Ocean1 --> ResponseParser --> ClientSub --> AgentApp
    Ocean1 -.-> S3Cache
                </div>
            </div>
        </section>

        <section>
            <h2>Complete ML Component Reference</h2>
            
            <h3>Core ML Services</h3>
            <table>
                <thead>
                    <tr><th>Component</th><th>Technology</th><th>Function</th><th>Latency Impact</th><th>Status</th></tr>
                </thead>
                <tbody>
                    <tr><td><strong>orchestrator</strong></td><td>Cresta Service</td><td>ML orchestration, prompt engineering, inference coordination</td><td>‚è±Ô∏è Critical path</td><td><span class="badge confirmed">‚úì Confirmed</span></td></tr>
                    <tr><td><strong>Event Consumer</strong></td><td>orchestrator component</td><td>Consumes transcript events from Redis</td><td>~10-50ms</td><td><span class="badge confirmed">‚úì Confirmed</span></td></tr>
                    <tr><td><strong>Prompt Builder</strong></td><td>orchestrator component</td><td>Constructs prompts with context and knowledge</td><td>~50-150ms</td><td><span class="badge confirmed">‚úì Confirmed</span></td></tr>
                    <tr><td><strong>Conversation History Manager</strong></td><td>orchestrator component</td><td>Retrieves recent conversation context from PostgreSQL</td><td>~20-100ms</td><td><span class="badge confirmed">‚úì Confirmed</span></td></tr>
                    <tr><td><strong>knowledgeretrieval</strong></td><td>Cresta Service</td><td>RAG service for knowledge base article retrieval</td><td>‚è±Ô∏è 100-300ms</td><td><span class="badge confirmed">‚úì Confirmed</span></td></tr>
                    <tr><td><strong>Embedding Generator</strong></td><td>knowledgeretrieval component</td><td>Generates semantic embeddings for queries</td><td>~50-150ms</td><td><span class="badge pending">üü° Model TBD</span></td></tr>
                    <tr><td><strong>Vector Search</strong></td><td>Pinecone query</td><td>Semantic similarity search across knowledge base</td><td>~50-100ms</td><td><span class="badge confirmed">‚úì Confirmed</span></td></tr>
                    <tr><td><strong>Result Reranker</strong></td><td>ML reranking model</td><td>Reranks top-k results for relevance</td><td>~50-100ms</td><td><span class="badge pending">üü° Model TBD</span></td></tr>
                    <tr><td><strong>Fireworks AI API</strong></td><td>Third-party service</td><td>Inference hosting for Ocean-1 model</td><td>‚è±Ô∏è 200-800ms</td><td><span class="badge confirmed">‚úì Confirmed</span></td></tr>
                    <tr><td><strong>Ocean-1</strong></td><td>Cresta Foundation Model</td><td>Contact center-specific LLM for suggestions</td><td>üß† Model inference</td><td><span class="badge confirmed">‚úì Confirmed</span></td></tr>
                    <tr><td><strong>Response Parser</strong></td><td>orchestrator component</td><td>Extracts suggestions, confidence scores from response</td><td>~10-30ms</td><td><span class="badge confirmed">‚úì Confirmed</span></td></tr>
                    <tr><td><strong>Suggestion Ranker</strong></td><td>orchestrator component</td><td>Ranks and filters suggestions by confidence</td><td>~10-30ms</td><td><span class="badge pending">üü° Logic TBD</span></td></tr>
                </tbody>
            </table>

            <h3>Data Storage Components</h3>
            <table>
                <thead>
                    <tr><th>Component</th><th>Technology</th><th>Purpose</th><th>Query Pattern</th><th>Status</th></tr>
                </thead>
                <tbody>
                    <tr><td><strong>PostgreSQL (Conversations)</strong></td><td>AWS RDS</td><td>Conversation history, transcripts, metadata</td><td>Read by conversation ID</td><td><span class="badge confirmed">‚úì Confirmed</span></td></tr>
                    <tr><td><strong>PostgreSQL (Knowledge Base)</strong></td><td>AWS RDS</td><td>Knowledge articles, FAQs, policies</td><td>Sync to Pinecone</td><td><span class="badge confirmed">‚úì Confirmed</span></td></tr>
                    <tr><td><strong>Pinecone</strong></td><td>Vector Database</td><td>Semantic search for knowledge retrieval</td><td>Vector similarity</td><td><span class="badge confirmed">‚úì Confirmed</span></td></tr>
                    <tr><td><strong>S3 Model Cache</strong></td><td>AWS S3</td><td>Model artifacts, prompt templates</td><td>Occasional read</td><td><span class="badge pending">üü° Verify</span></td></tr>
                    <tr><td><strong>Redis (Events)</strong></td><td>AWS ElastiCache</td><td>Real-time event streaming</td><td>Pub/sub</td><td><span class="badge confirmed">‚úì Confirmed</span></td></tr>
                </tbody>
            </table>
        </section>

        <section>
            <h2>Ocean-1 Foundation Model</h2>

            <div class="highlight-box success">
                <h3>Model Overview</h3>
                <p><strong>Ocean-1</strong> is Cresta's proprietary foundation model trained specifically for contact center conversations. It understands telephony context, agent-customer dynamics, and business-specific language patterns. The model is hosted and served by <strong>Fireworks AI</strong>, a specialized inference platform optimized for low-latency LLM serving.</p>
            </div>

            <h3>Ocean-1 Specifications</h3>
            <table>
                <thead>
                    <tr><th>Attribute</th><th>Value</th><th>Notes</th><th>Status</th></tr>
                </thead>
                <tbody>
                    <tr><td><strong>Model Family</strong></td><td>Ocean-1</td><td>Proprietary Cresta foundation model</td><td><span class="badge confirmed">‚úì Confirmed</span></td></tr>
                    <tr><td><strong>Architecture</strong></td><td>Transformer-based</td><td>Likely GPT-style decoder architecture</td><td><span class="badge pending">üü° Verify</span></td></tr>
                    <tr><td><strong>Context Window</strong></td><td>üü° Unknown</td><td>Typical range: 4K-32K tokens</td><td><span class="badge pending">üü° Verify</span></td></tr>
                    <tr><td><strong>Training Data</strong></td><td>Contact center conversations</td><td>Includes customer service, sales, support</td><td><span class="badge confirmed">‚úì Confirmed</span></td></tr>
                    <tr><td><strong>Specialization</strong></td><td>Agent assistance, suggestion generation</td><td>Optimized for real-time guidance</td><td><span class="badge confirmed">‚úì Confirmed</span></td></tr>
                    <tr><td><strong>Inference Provider</strong></td><td>Fireworks AI</td><td>Third-party hosting partnership</td><td><span class="badge confirmed">‚úì Confirmed</span></td></tr>
                    <tr><td><strong>Model Updates</strong></td><td>üü° Unknown</td><td>Frequency of model version releases</td><td><span class="badge pending">üü° Verify</span></td></tr>
                    <tr><td><strong>Fine-tuning Support</strong></td><td>üü° Unknown</td><td>Customer-specific model customization</td><td><span class="badge pending">üü° Verify</span></td></tr>
                </tbody>
            </table>

            <h3>Fireworks AI Partnership</h3>
            <table>
                <thead>
                    <tr><th>Aspect</th><th>Details</th><th>Benefit</th><th>Status</th></tr>
                </thead>
                <tbody>
                    <tr><td><strong>Infrastructure</strong></td><td>Fireworks AI inference platform</td><td>Optimized for low-latency serving</td><td><span class="badge confirmed">‚úì Confirmed</span></td></tr>
                    <tr><td><strong>Typical Latency</strong></td><td>200-800ms (including network)</td><td>Suitable for real-time assistance</td><td><span class="badge confirmed">‚úì Cresta Blog</span></td></tr>
                    <tr><td><strong>Scaling</strong></td><td>Auto-scaling inference capacity</td><td>Handles peak traffic</td><td><span class="badge pending">üü° SLA TBD</span></td></tr>
                    <tr><td><strong>Availability</strong></td><td>üü° Unknown SLA</td><td>Need to verify uptime guarantees</td><td><span class="badge pending">üü° Verify</span></td></tr>
                    <tr><td><strong>Geographic Distribution</strong></td><td>üü° Unknown regions</td><td>Latency depends on proximity</td><td><span class="badge pending">üü° Verify</span></td></tr>
                    <tr><td><strong>Fallback Strategy</strong></td><td>üü° Unknown</td><td>What happens if Fireworks unavailable?</td><td><span class="badge pending">üü° Verify</span></td></tr>
                </tbody>
            </table>

            <div class="highlight-box info">
                <h3>Why Fireworks AI?</h3>
                <p>Fireworks AI specializes in high-performance LLM inference with focus on:</p>
                <ul style="padding-left: 24px; color: var(--text-secondary); line-height: 1.8;">
                    <li><strong>Low Latency:</strong> Optimized for real-time applications (&lt;1s p95)</li>
                    <li><strong>High Throughput:</strong> Handles thousands of concurrent requests</li>
                    <li><strong>Cost Efficiency:</strong> Competitive pricing vs self-hosting or other providers</li>
                    <li><strong>Model Optimization:</strong> Custom kernels and quantization for speed</li>
                    <li><strong>Multi-Tenancy:</strong> Secure isolation for enterprise customers</li>
                </ul>
            </div>
        </section>

        <section>
            <h2>Prompt Engineering & Context Assembly</h2>

            <div class="diagram-container">
                <div class="diagram-title">Prompt Construction Flow</div>
                <div class="mermaid">
sequenceDiagram
    participant Event as Transcript Event
    participant Orch as orchestrator
    participant History as Conversation History
    participant KB as knowledgeretrieval
    participant Ocean as Ocean-1 (Fireworks)
    participant Agent as Agent App

    Event->>Orch: New utterance event
    activate Orch
    
    Orch->>History: Get recent conversation (last N turns)
    History-->>Orch: Previous utterances + context
    
    Orch->>KB: Query for relevant knowledge
    activate KB
    KB->>KB: Generate embedding
    KB->>KB: Vector search (Pinecone)
    KB->>KB: Rerank results
    KB-->>Orch: Top 3-5 knowledge articles
    deactivate KB
    
    Orch->>Orch: Build prompt:<br/>‚Ä¢ System instructions<br/>‚Ä¢ Knowledge articles<br/>‚Ä¢ Conversation history<br/>‚Ä¢ Current utterance
    
    Orch->>Ocean: POST /inference
    activate Ocean
    Ocean->>Ocean: Model inference
    Ocean-->>Orch: Suggestions + confidence
    deactivate Ocean
    
    Orch->>Orch: Parse & rank suggestions
    Orch->>Agent: Publish via clientsubscription
    deactivate Orch
                </div>
            </div>

            <h3>Prompt Template Structure</h3>
            <div class="code-block">
                <pre>SYSTEM: You are an expert contact center AI assistant. Provide concise, 
actionable suggestions to help the agent resolve the customer's issue.

KNOWLEDGE BASE:
[Article 1: How to Process Refunds]
- Steps: 1) Verify order number, 2) Check eligibility, 3) Initiate refund...

[Article 2: Return Policy]
- Eligible within 30 days of purchase with receipt...

CONVERSATION HISTORY:
Customer: "I received a damaged product and want a refund."
Agent: "I'm sorry to hear that. Can you provide your order number?"
Customer: "Yes, it's ORDER-12345."

CURRENT UTTERANCE:
Customer: "How long will the refund take?"

INSTRUCTION: Based on the knowledge base and conversation, suggest the 
next best response for the agent.</pre>
            </div>

            <h3>Prompt Components</h3>
            <table>
                <thead>
                    <tr><th>Component</th><th>Purpose</th><th>Typical Size</th><th>Source</th></tr>
                </thead>
                <tbody>
                    <tr><td><strong>System Instructions</strong></td><td>Define model role and behavior</td><td>100-300 tokens</td><td>Prompt template</td></tr>
                    <tr><td><strong>Knowledge Articles</strong></td><td>Provide relevant context and policies</td><td>500-2000 tokens</td><td>RAG retrieval (knowledgeretrieval)</td></tr>
                    <tr><td><strong>Conversation History</strong></td><td>Recent turns for context continuity</td><td>300-1000 tokens</td><td>PostgreSQL conversation store</td></tr>
                    <tr><td><strong>Current Utterance</strong></td><td>Latest customer or agent statement</td><td>20-100 tokens</td><td>Real-time transcript</td></tr>
                    <tr><td><strong>Agent Profile</strong></td><td>Agent skill level, preferences</td><td>50-200 tokens</td><td>Agent metadata (üü° verify)</td></tr>
                    <tr><td><strong>Task Instruction</strong></td><td>Specific guidance request</td><td>50-150 tokens</td><td>Prompt template</td></tr>
                </tbody>
            </table>

            <h3>Conversation History Window üü°</h3>
            <table>
                <thead>
                    <tr><th>Parameter</th><th>Typical Value</th><th>Trade-off</th><th>Status</th></tr>
                </thead>
                <tbody>
                    <tr><td><strong>History Turns</strong></td><td>5-10 turns (10-20 utterances)</td><td>Context vs. token cost</td><td><span class="badge pending">üü° Verify</span></td></tr>
                    <tr><td><strong>Token Limit</strong></td><td>~1000 tokens for history</td><td>Prevents context overflow</td><td><span class="badge pending">üü° Verify</span></td></tr>
                    <tr><td><strong>Summary Usage</strong></td><td>Summarize older turns if needed</td><td>Compression vs. detail</td><td><span class="badge pending">üü° Verify</span></td></tr>
                </tbody>
            </table>
        </section>

        <section>
            <h2>Knowledge Retrieval (RAG)</h2>

            <div class="context-box">
                <strong>Context:</strong> The <code>knowledgeretrieval</code> service implements Retrieval-Augmented Generation (RAG) to find relevant knowledge base articles. It generates semantic embeddings for queries, performs vector similarity search in Pinecone, and reranks results before returning the top 3-5 articles to the orchestrator for inclusion in prompts.
            </div>

            <div class="diagram-container">
                <div class="diagram-title">RAG Pipeline Architecture</div>
                <div class="mermaid">
flowchart LR
    subgraph Input["Query Input"]
        UserQuery["Customer Utterance:<br/>'How do I return<br/>a damaged item?'"]
    end

    subgraph EmbeddingGen["Embedding Generation"]
        EmbedModel["Embedding Model<br/>(e.g., text-embedding-ada-002)"]
        QueryVector["Query Vector<br/>[0.123, -0.456, ...]"]
    end

    subgraph VectorDB["Vector Database - Pinecone"]
        Index["Knowledge Base<br/>Index"]
        SimilaritySearch["Cosine<br/>Similarity"]
        TopK["Top 10<br/>Results"]
    end

    subgraph Reranking["Reranking"]
        RerankerModel["Reranker Model<br/>(e.g., cross-encoder)"]
        TopN["Top 3-5<br/>Articles"]
    end

    subgraph Output["Output"]
        Articles["Relevant Articles:<br/>1. Return Policy<br/>2. Damage Claims<br/>3. Refund Process"]
    end

    UserQuery --> EmbedModel --> QueryVector
    QueryVector --> Index --> SimilaritySearch --> TopK
    TopK --> RerankerModel --> TopN --> Articles
                </div>
            </div>

            <h3>RAG Component Specifications</h3>
            <table>
                <thead>
                    <tr><th>Component</th><th>Technology</th><th>Latency</th><th>Purpose</th><th>Status</th></tr>
                </thead>
                <tbody>
                    <tr><td><strong>Embedding Model</strong></td><td>üü° Unknown (likely OpenAI or custom)</td><td>~50-150ms</td><td>Convert queries to vectors</td><td><span class="badge pending">üü° Verify</span></td></tr>
                    <tr><td><strong>Vector Database</strong></td><td>Pinecone</td><td>~50-100ms</td><td>Semantic similarity search</td><td><span class="badge confirmed">‚úì Confirmed</span></td></tr>
                    <tr><td><strong>Reranker Model</strong></td><td>üü° Unknown (likely cross-encoder)</td><td>~50-100ms</td><td>Improve ranking quality</td><td><span class="badge pending">üü° Verify</span></td></tr>
                    <tr><td><strong>Knowledge Ingestion</strong></td><td>Batch process from PostgreSQL</td><td>Offline</td><td>Sync KB to Pinecone</td><td><span class="badge pending">üü° Frequency TBD</span></td></tr>
                </tbody>
            </table>

            <h3>Knowledge Base Structure</h3>
            <table>
                <thead>
                    <tr><th>Field</th><th>Type</th><th>Purpose</th><th>Example</th></tr>
                </thead>
                <tbody>
                    <tr><td><strong>Article ID</strong></td><td>UUID</td><td>Unique identifier</td><td><code>kb-12345</code></td></tr>
                    <tr><td><strong>Title</strong></td><td>String</td><td>Article headline</td><td>"How to Process Returns"</td></tr>
                    <tr><td><strong>Content</strong></td><td>Text</td><td>Article body (markdown/HTML)</td><td>"Steps: 1) Verify order..."</td></tr>
                    <tr><td><strong>Category</strong></td><td>String</td><td>Topic classification</td><td>"Returns & Refunds"</td></tr>
                    <tr><td><strong>Tags</strong></td><td>Array</td><td>Search keywords</td><td>["return", "refund", "damaged"]</td></tr>
                    <tr><td><strong>Embedding</strong></td><td>Vector (768-1536 dim)</td><td>Semantic representation</td><td>[0.123, -0.456, ...]</td></tr>
                    <tr><td><strong>Last Updated</strong></td><td>Timestamp</td><td>Version tracking</td><td>2025-01-15T10:30:00Z</td></tr>
                </tbody>
            </table>

            <h3>RAG Performance Metrics</h3>
            <table>
                <thead>
                    <tr><th>Metric</th><th>Target</th><th>Measurement</th><th>Impact</th></tr>
                </thead>
                <tbody>
                    <tr><td><strong>Retrieval Latency</strong></td><td>&lt;300ms</td><td>End-to-end query time</td><td>‚è±Ô∏è Real-time constraint</td></tr>
                    <tr><td><strong>Recall@10</strong></td><td>&gt;90%</td><td>Relevant article in top 10</td><td>Ensures correct context</td></tr>
                    <tr><td><strong>Precision@3</strong></td><td>&gt;80%</td><td>Top 3 articles are relevant</td><td>Reduces noise in prompts</td></tr>
                    <tr><td><strong>Reranker Improvement</strong></td><td>+10-20% precision</td><td>After reranking vs before</td><td>Justifies reranking cost</td></tr>
                </tbody>
            </table>
        </section>

        <section>
            <h2>ML Inference Latency Budget</h2>

            <div class="highlight-box info">
                <h3>Total Latency Target: &lt;1.5 seconds</h3>
                <p>From customer speech to agent seeing suggestion. Breakdown by component:</p>
            </div>

            <table>
                <thead>
                    <tr><th>Stage</th><th>Component</th><th>Target (ms)</th><th>p95 (ms)</th><th>Risk</th></tr>
                </thead>
                <tbody>
                    <tr><td><strong>1. ASR</strong></td><td>Deepgram ASR</td><td>200-300</td><td>~400</td><td><span class="badge high">High</span></td></tr>
                    <tr><td><strong>2. Event Delivery</strong></td><td>Redis ‚Üí orchestrator</td><td>10-50</td><td>~80</td><td><span class="badge low">Low</span></td></tr>
                    <tr><td><strong>3. History Retrieval</strong></td><td>PostgreSQL query</td><td>20-100</td><td>~150</td><td><span class="badge low">Low</span></td></tr>
                    <tr><td><strong>4. Knowledge Retrieval</strong></td><td>knowledgeretrieval (RAG)</td><td>100-300</td><td>~400</td><td><span class="badge medium">Medium</span></td></tr>
                    <tr><td><strong>5. Prompt Construction</strong></td><td>orchestrator logic</td><td>50-150</td><td>~200</td><td><span class="badge low">Low</span></td></tr>
                    <tr><td><strong>6. ML Inference</strong></td><td>Fireworks AI (Ocean-1)</td><td>200-800</td><td>~1000</td><td><span class="badge high">High</span></td></tr>
                    <tr><td><strong>7. Response Parsing</strong></td><td>orchestrator logic</td><td>10-30</td><td>~50</td><td><span class="badge low">Low</span></td></tr>
                    <tr><td><strong>8. Agent Delivery</strong></td><td>clientsubscription WebSocket</td><td>20-100</td><td>~150</td><td><span class="badge low">Low</span></td></tr>
                    <tr><td><strong>TOTAL</strong></td><td>‚Äî</td><td><strong>610-1830</strong></td><td><strong>~2430</strong></td><td><span class="badge high">Monitor</span></td></tr>
                </tbody>
            </table>

            <div class="highlight-box danger">
                <h3>‚ö†Ô∏è Latency Risks</h3>
                <ul style="padding-left: 24px; color: var(--text-secondary); line-height: 1.8;">
                    <li><strong>Critical Path Dependencies:</strong> ASR (Deepgram) and ML Inference (Fireworks) are external services with no direct control</li>
                    <li><strong>p95 Exceeds Target:</strong> The p95 latency (~2.4s) exceeds the 1.5s target, indicating tail latency issues</li>
                    <li><strong>Cascading Delays:</strong> If knowledge retrieval and inference both hit p95, total latency could exceed 3 seconds</li>
                    <li><strong>No Caching Mentioned:</strong> üü° Verify if prompt caching or response caching is used to reduce inference latency</li>
                </ul>
            </div>
        </section>

        <section>
            <h2>Suggestion Types & Confidence Scoring</h2>

            <h3>Suggestion Categories</h3>
            <table>
                <thead>
                    <tr><th>Type</th><th>Description</th><th>Example</th><th>Confidence Threshold</th></tr>
                </thead>
                <tbody>
                    <tr><td><strong>Next Best Action</strong></td><td>What the agent should do next</td><td>"Ask for the order number to look up the transaction"</td><td>&gt;70%</td></tr>
                    <tr><td><strong>Scripted Response</strong></td><td>Exact phrasing suggestion</td><td>"I'd be happy to help you with that. Let me pull up your account."</td><td>&gt;80%</td></tr>
                    <tr><td><strong>Knowledge Snippet</strong></td><td>Relevant policy or procedure</td><td>"Refunds typically take 5-7 business days to process"</td><td>&gt;60%</td></tr>
                    <tr><td><strong>Objection Handling</strong></td><td>How to address customer concerns</td><td>"Acknowledge their frustration, then explain the resolution process"</td><td>&gt;75%</td></tr>
                    <tr><td><strong>De-escalation</strong></td><td>Techniques to calm angry customers</td><td>"Use empathetic language and offer to escalate to a supervisor"</td><td>&gt;85%</td></tr>
                    <tr><td><strong>Upsell/Cross-sell</strong></td><td>Sales opportunities</td><td>"Based on their purchase, suggest the extended warranty"</td><td>&gt;65%</td></tr>
                    <tr><td><strong>Compliance Warning</strong></td><td>Regulatory or policy alerts</td><td>"‚ö†Ô∏è Do not disclose social security numbers over the phone"</td><td>&gt;95%</td></tr>
                </tbody>
            </table>

            <h3>Confidence Score Interpretation</h3>
            <table>
                <thead>
                    <tr><th>Score Range</th><th>Label</th><th>Action</th><th>Display</th></tr>
                </thead>
                <tbody>
                    <tr><td><strong>95-100%</strong></td><td>Very High</td><td>Auto-display prominently</td><td>üü¢ Green badge</td></tr>
                    <tr><td><strong>80-94%</strong></td><td>High</td><td>Display as primary suggestion</td><td>üîµ Blue badge</td></tr>
                    <tr><td><strong>65-79%</strong></td><td>Medium</td><td>Display as secondary option</td><td>üü° Yellow badge</td></tr>
                    <tr><td><strong>50-64%</strong></td><td>Low</td><td>Show only if no higher options</td><td>‚ö™ Gray badge</td></tr>
                    <tr><td><strong>&lt;50%</strong></td><td>Very Low</td><td>Suppress (do not display)</td><td>‚ùå Not shown</td></tr>
                </tbody>
            </table>
        </section>

        <section>
            <h2>Batch Processing & Summarization</h2>

            <div class="context-box">
                <strong>Context:</strong> In addition to real-time inference, Ocean-1 performs end-of-call summarization and batch analytics. After a conversation ends, the full transcript is processed to generate summaries, identify key moments, extract action items, and analyze sentiment.
            </div>

            <h3>Batch Processing Use Cases</h3>
            <table>
                <thead>
                    <tr><th>Use Case</th><th>Trigger</th><th>Latency</th><th>Output</th></tr>
                </thead>
                <tbody>
                    <tr><td><strong>Call Summary</strong></td><td>End of call</td><td>3-8 seconds</td><td>150-300 word summary of conversation</td></tr>
                    <tr><td><strong>Key Moments</strong></td><td>End of call</td><td>2-5 seconds</td><td>Timestamps of objections, commitments, escalations</td></tr>
                    <tr><td><strong>Action Items</strong></td><td>End of call</td><td>2-4 seconds</td><td>Extracted tasks (e.g., "Follow up in 3 days")</td></tr>
                    <tr><td><strong>Sentiment Analysis</strong></td><td>End of call</td><td>1-2 seconds</td><td>Customer satisfaction score (1-5)</td></tr>
                    <tr><td><strong>Quality Assurance</strong></td><td>End of call</td><td>5-10 seconds</td><td>Compliance checks, script adherence</td></tr>
                    <tr><td><strong>Coaching Insights</strong></td><td>Daily batch</td><td>Minutes</td><td>Agent performance trends, improvement areas</td></tr>
                    <tr><td><strong>Knowledge Gap Detection</strong></td><td>Weekly batch</td><td>Hours</td><td>Missing KB articles, frequently asked questions</td></tr>
                </tbody>
            </table>

            <h3>Batch Processing Architecture</h3>
            <div class="diagram-container">
                <div class="diagram-title">End-of-Call Batch Processing</div>
                <div class="mermaid">
flowchart LR
    subgraph Trigger["Call End Trigger"]
        CloseEvent["CloseConversation<br/>Event"]
    end

    subgraph BatchOrch["Batch orchestrator"]
        ConvLoader["Load Full<br/>Transcript"]
        TaskScheduler["Schedule<br/>Batch Tasks"]
    end

    subgraph InferenceJobs["Parallel Inference"]
        Summarization["Summarization<br/>Job"]
        KeyMoments["Key Moments<br/>Job"]
        Sentiment["Sentiment<br/>Job"]
        QA["QA Checks<br/>Job"]
    end

    subgraph Results["Results Storage"]
        PostgresResults[("PostgreSQL<br/>Conversation Metadata")]
        ClickHouseAnalytics[("ClickHouse<br/>Analytics")]
        ESIndex[("Elasticsearch<br/>Search Index")]
    end

    CloseEvent --> ConvLoader --> TaskScheduler
    TaskScheduler --> Summarization
    TaskScheduler --> KeyMoments
    TaskScheduler --> Sentiment
    TaskScheduler --> QA
    Summarization --> PostgresResults
    KeyMoments --> PostgresResults
    Sentiment --> ClickHouseAnalytics
    QA --> ESIndex
                </div>
            </div>
        </section>

        <section>
            <h2>Risk Assessment</h2>

            <div class="card-grid">
                <div class="card latency">
                    <h4>‚è±Ô∏è Latency Risks</h4>
                    <table>
                        <tr><td>Fireworks AI Inference</td><td><span class="badge high">High</span></td><td>200-800ms target, p95 ~1000ms</td></tr>
                        <tr><td>Knowledge Retrieval (RAG)</td><td><span class="badge medium">Medium</span></td><td>100-300ms, depends on Pinecone</td></tr>
                        <tr><td>Network Latency</td><td><span class="badge medium">Medium</span></td><td>Geographic distance to Fireworks</td></tr>
                        <tr><td>Total p95 Latency</td><td><span class="badge high">High</span></td><td>~2.4s exceeds 1.5s target</td></tr>
                    </table>
                    <p style="margin-top: 12px; font-size: 0.875rem;"><strong>Mitigation:</strong> Regional deployment, prompt caching, response caching, optimize prompt length</p>
                </div>

                <div class="card cost">
                    <h4>üí∞ Cost Risks</h4>
                    <table>
                        <tr><td>Inference per Call</td><td><span class="badge pending">üü° TBD</span></td><td>Ocean-1 via Fireworks AI</td></tr>
                        <tr><td>Knowledge Embedding</td><td><span class="badge low">Low</span></td><td>Amortized over many queries</td></tr>
                        <tr><td>Vector Search</td><td><span class="badge low">Low</span></td><td>Pinecone pricing per query</td></tr>
                        <tr><td>Data Storage</td><td><span class="badge low">Low</span></td><td>PostgreSQL, S3, Redis</td></tr>
                    </table>
                    <p style="margin-top: 12px; font-size: 0.875rem;"><strong>Mitigation:</strong> üü° Verify Fireworks AI pricing model and estimate costs</p>
                </div>

                <div class="card operational">
                    <h4>‚öôÔ∏è Operational Risks</h4>
                    <table>
                        <tr><td>Fireworks Availability</td><td><span class="badge pending">üü° Verify</span></td><td>SLA and failover strategy unknown</td></tr>
                        <tr><td>Model Drift</td><td><span class="badge medium">Medium</span></td><td>Performance degradation over time</td></tr>
                        <tr><td>Prompt Injection</td><td><span class="badge medium">Medium</span></td><td>Customer manipulating model output</td></tr>
                        <tr><td>Knowledge Staleness</td><td><span class="badge low">Low</span></td><td>KB updates lag production</td></tr>
                    </table>
                    <p style="margin-top: 12px; font-size: 0.875rem;"><strong>Mitigation:</strong> Monitoring, model versioning, input sanitization, regular KB sync</p>
                </div>
            </div>
        </section>

        <section>
            <h2>Items Requiring Follow-up üü°</h2>

            <div class="highlight-box danger">
                <h4>üî¥ High Priority (Integration Critical)</h4>
                <ol style="padding-left: 20px; color: var(--text-secondary); line-height: 1.8;">
                    <li><strong>Fireworks AI SLA:</strong> What is the uptime SLA? What happens if Fireworks AI is unavailable? Is there a fallback inference provider?</li>
                    <li><strong>Ocean-1 Context Window:</strong> What is the maximum context window size? How is context truncated if conversation history exceeds limit?</li>
                    <li><strong>Inference Pricing:</strong> What is the cost per inference call? Token-based or request-based pricing? Expected monthly spend for target call volume?</li>
                    <li><strong>Model Updates:</strong> How frequently is Ocean-1 updated? What is the process for model version rollout and testing?</li>
                    <li><strong>Fine-tuning Support:</strong> Can Ocean-1 be fine-tuned with customer-specific data? What is the process and timeline?</li>
                </ol>
            </div>

            <div class="highlight-box info">
                <h4>üü° Medium Priority (Operational & Optimization)</h4>
                <ol start="6" style="padding-left: 20px; color: var(--text-secondary); line-height: 1.8;">
                    <li><strong>Embedding Model:</strong> What embedding model is used for knowledge retrieval? OpenAI, Cohere, or custom?</li>
                    <li><strong>Reranker Model:</strong> What reranking model is used? How does it improve precision?</li>
                    <li><strong>Conversation History Length:</strong> How many turns are included in prompts? Is there token-based or turn-based limiting?</li>
                    <li><strong>Caching Strategy:</strong> Are prompt embeddings or inference responses cached? What is the cache hit rate?</li>
                    <li><strong>Geographic Distribution:</strong> Where are Fireworks AI inference endpoints located? How does this affect latency for different regions?</li>
                    <li><strong>Knowledge Base Sync:</strong> How frequently is the knowledge base re-embedded and synced to Pinecone?</li>
                    <li><strong>Model Monitoring:</strong> What metrics are tracked for model performance? How is drift detected?</li>
                    <li><strong>Prompt Injection Protection:</strong> What safeguards prevent customers from manipulating model outputs via adversarial inputs?</li>
                </ol>
            </div>
        </section>
    </div>

    <footer>
        <div class="container">
            <div>Cresta AI Platform Assessment - Document 04: ML Services & Inference Architecture</div>
            <div>January 2025 | Version 2.0</div>
        </div>
    </footer>

    <script>
        mermaid.initialize({
            startOnLoad: true,
            theme: 'dark',
            themeVariables: {
                primaryColor: '#a855f7', primaryTextColor: '#f1f5f9', primaryBorderColor: '#334155',
                lineColor: '#64748b', secondaryColor: '#1e293b', tertiaryColor: '#0f172a',
                background: '#1e293b', mainBkg: '#1e293b', nodeBorder: '#334155',
                clusterBkg: '#1e293b', clusterBorder: '#475569'
            },
            flowchart: { useMaxWidth: true, htmlLabels: true, curve: 'basis' },
            sequence: { useMaxWidth: true },
            securityLevel: 'loose'
        });
    </script>
</body>
</html>
