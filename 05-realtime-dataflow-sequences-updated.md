# Cresta Real-Time Data Flow Sequences

## Document Purpose

This document traces end-to-end data flows through the Cresta platform, showing how audio becomes actionable agent guidance. All sequences are based on verified architecture components from previous documents.

---

## Complete Call Flow: Customer Speech to Agent Guidance

### Phase 1: Audio Capture & Streaming (0-100ms)

```
Customer speaks
   â†“ (Telephony network)
Amazon Connect Instance
   â†“ (Contact Flow: Start Media Streaming block)
Kinesis Video Streams
   â”œâ”€ Creates dedicated stream (one per call)
   â”œâ”€ 8kHz PCM audio
   â”œâ”€ AUDIO_FROM_CUSTOMER track
   â””â”€ AUDIO_TO_CUSTOMER track
   â†“ (WebSocket/HTTP2, StreamARN passed via Contact Attributes)
Customer-specific subdomain (customer.region.cresta.ai)
   â†“ (AWS ELB â†’ NGINX Ingress + Wallarm WAF)
gowalter WebSocket Handler
```

**Cumulative Latency**: ~50-100ms
**Verified**: AWS Connect docs, Cresta blog

---

### Phase 2: ASR Transcription (100-400ms)

```
gowalter
   â”œâ”€ Audio Buffer (accumulates 20-100ms chunks)
   â”œâ”€ WebSocket to Deepgram ASR
   â””â”€ Recovery Manager (replays on disconnect)
   â†“ (20-100ms chunks streamed)
Deepgram Streaming ASR (Nova-3)
   â”œâ”€ Partial transcripts (every 0.5-1.5s)
   â”œâ”€ Final transcripts (every 3-7s)
   â””â”€ Phonetic corrections as context arrives
   â†“ (Sub-300ms latency)
gowalter Utterance Builder
   â”œâ”€ Organizes ASR chunks into conversation units
   â”œâ”€ Speaker diarization (customer vs agent)
   â””â”€ Prepares for persistence
   â†“
apiserver Transcript Upsert
   â”œâ”€ PostgreSQL insert (~10-50ms)
   â”œâ”€ Event publication to Redis (~5-20ms)
   â””â”€ Approximately 1 update/second to agent
```

**Cumulative Latency**: ~150-500ms
**Verified**: Deepgram benchmarks, Cresta blog on voice stack

---

### Phase 3: ML Processing (Variable, 50-600ms)

```
apiserver Event Publisher
   â†“ (Redis Streams, asynchronous)
Orchestrator
   â”œâ”€ Loads cached search policies
   â”‚  â””â”€ Customer/team/agent-specific configuration
   â”œâ”€ Builds inference graph
   â”‚  â””â”€ Blueprint of which models to call
   â””â”€ Routes to ML services
   â†“ (Routing decision ~5-20ms)
ML Router
   â”œâ”€ Selects appropriate model shards
   â”œâ”€ Batch mode (if burst traffic): 100-300ms window
   â””â”€ Real-time mode (if live serving): immediate forwarding
   â†“
Model Shards (Kubernetes Pods)
   â”œâ”€ Envelope (intra-pod router) <5ms
   â”œâ”€ Local models (classifiers: ~50-200ms)
   â”‚  â”œâ”€ Intent detection
   â”‚  â”œâ”€ Sentiment analysis
   â”‚  â””â”€ Entity extraction
   â””â”€ Ocean-1 + LoRA (via Fireworks AI)
      â”œâ”€ Knowledge Assist (RAG)
      â”œâ”€ Response suggestions
      â””â”€ Policy compliance checks
   â†“
Annotations Generated
   â”œâ”€ Moments (detections, alerts)
   â””â”€ Actions (suggestions, hints)
```

**ML Processing Latency**:
- Lightweight classifiers: 50-200ms
- Ocean-1 RAG: Variable (ðŸŸ¡ needs benchmarks)
- Batching adds: 100-300ms if triggered

**Verified**: Cresta blog on ML services, Fireworks partnership

---

### Phase 4: Real-Time Delivery to Agent (50-150ms)

```
Orchestrator
   â†“ (Annotations aggregated)
clientsubscription Service
   â”œâ”€ Redis Streams pub/sub (~5-20ms)
   â”œâ”€ WebSocket to Agent App
   â””â”€ Approximately 1 update/second delivery
   â†“
Cresta Agent App (Desktop)
   â”œâ”€ Renders guidance in UI (~50-100ms)
   â”œâ”€ Displays real-time transcript
   â”œâ”€ Shows actions (knowledge articles, suggestions)
   â””â”€ Highlights moments (compliance alerts, sentiment)
   â†“
Agent sees guidance on screen
```

**Cumulative Latency**: ~55-170ms
**Verified**: Cresta blog on voice stack and clientsubscription

---

## End-to-End Latency Budget

| Stage | Component | Latency (Verified/Estimated) |
|-------|-----------|------------------------------|
| 1 | Audio capture (Connect) | ~20-50ms |
| 2 | KVS ingestion | ~20-30ms |
| 3 | Network transit | ~30-100ms |
| 4 | gowalter buffering | 20-100ms |
| 5 | **Deepgram ASR** | **<300ms** âœ… |
| 6 | Utterance building | ~100-200ms |
| 7 | PostgreSQL upsert | ~10-50ms |
| 8 | Redis event pub | ~5-20ms |
| 9 | Orchestrator processing | ~5-20ms |
| 10 | ML routing | ~5-20ms |
| 11 | **ML inference (light)** | **50-200ms** |
| 11a | **ML inference (Ocean-1)** | **ðŸŸ¡ Variable** |
| 11b | **Batching (if triggered)** | **+100-300ms** |
| 12 | Annotation aggregation | ~10-30ms |
| 13 | Redis to clientsub | ~5-20ms |
| 14 | Agent App render | ~50-100ms |

**Best Case Total** (lightweight models, no batching): ~530ms - 820ms âœ…
**Typical Case** (Ocean-1 RAG): ~750ms - 1,500ms (assuming ~200-500ms Ocean-1 latency)
**Worst Case** (batching + complex ML): ~1,250ms - 2,120ms

**Target**: <1,500ms (industry standard for voice assistance)
**Confidence**: High for ASR, Medium for ML (needs vendor benchmarks)

---

## Parallel End-of-Call Processing

### Triggered When: Call Disconnects

```
gowalter Detects Call End
   â”œâ”€ Final Audio Processing
   â”‚  â”œâ”€ Audio Redaction Engine (PII beeps)
   â”‚  â”œâ”€ Audio Encoder (compression)
   â”‚  â””â”€ S3 Upload (redacted audio)
   â”‚
   â””â”€ Async processing, no impact on call
       
apiserver Receives CloseConversation Event
   â”œâ”€ Elasticsearch Indexing
   â”‚  â””â”€ Fast search capability for historical lookup
   â”œâ”€ ClickHouse Storage  
   â”‚  â””â”€ Analytics data warehouse
   â”œâ”€ Temporal Workflow Launch
   â”‚  â””â”€ PII Redaction Verification
   â”‚     â”œâ”€ Double-check audio redaction complete
   â”‚     â”œâ”€ Verify text redaction complete
   â”‚     â””â”€ Ensure no un-redacted PII remains
   â””â”€ End-of-Call Features
      â”œâ”€ Conversation Summarization
      â”œâ”€ Quality Scoring
      â””â”€ Analytics Computation
```

**Processing Time**: Async (minutes), no impact on real-time performance
**Verified**: Cresta blog on ML services and voice stack

---

## Critical Path Analysis

### Latency-Critical Components (Must Optimize)

1. **Deepgram ASR**: <300ms âœ… (Verified best-in-class)
2. **Ocean-1 Inference**: Variable âš ï¸ (Needs benchmarks from Fireworks)
3. **gowalter Buffering**: 20-100ms âœ… (Optimized chunk size)
4. **Batcher Window**: 100-300ms âš ï¸ (Trade-off: latency vs throughput)

### Non-Critical Path (Async Acceptable)

1. **S3 Audio Storage**: End-of-call, async
2. **Elasticsearch Indexing**: End-of-call, async
3. **ClickHouse Analytics**: End-of-call, async
4. **Temporal Verification**: End-of-call, async

---

## Data Flow Variations

### Variation 1: Agent App Audio Capture (CCaaS Without Streaming)

```
Agent Desktop
   â†“ (System audio capture: mic + speakers)
Cresta Agent App
   â†“ (Direct upload to Cresta)
gowalter WebSocket Handler
   â†“ (Continue standard flow from Phase 2)
```

**Use Case**: CCaaS platforms that don't provide native audio streaming APIs
**Note**: Amazon Connect supports native streaming, so this is fallback only

---

### Variation 2: Batch ML Processing (High Traffic)

```
Multiple Transcript Updates (burst traffic)
   â†“
ML Router Detects Pattern
   â†“ (Activates Batcher)
Batcher (100-300ms collection window)
   â”œâ”€ Aggregates requests by model type
   â””â”€ Sends batched requests to shards
   â†“
GPU Batch Inference
   â”œâ”€ Process multiple requests simultaneously
   â”œâ”€ Higher throughput (~2-5x)
   â””â”€ Better GPU utilization
   â†“ (Results returned for batch)
Individual Annotations Distributed
   â†“ (To respective conversations)
Standard delivery to Agent Apps
```

**Trade-off**:
- **Latency**: +100-300ms from batching window
- **Throughput**: 2-5x increase in requests/second
- **Cost**: Improved GPU efficiency

**Trigger**: High traffic periods (call center peak hours)

---

## Monitoring Recommended Metrics

### Real-Time Performance (SLA Compliance)

| Metric | Target | Priority | Where to Measure |
|--------|--------|----------|------------------|
| ASR latency (p50) | <300ms | Critical | gowalter â†’ Deepgram |
| ASR latency (p99) | <500ms | Critical | gowalter â†’ Deepgram |
| ML inference (p50) | <200ms | High | Router â†’ Shard â†’ Response |
| ML inference (p99) | <500ms | High | Router â†’ Shard â†’ Response |
| End-to-end (p50) | <1,000ms | Critical | Speech â†’ Agent screen |
| End-to-end (p99) | <1,500ms | Critical | Speech â†’ Agent screen |
| Agent update frequency | ~1/sec | Medium | clientsubscription delivery |

### Operational Health

| Metric | Target | Priority | Where to Measure |
|--------|--------|----------|------------------|
| gowalter recovery events | <1% | Medium | WebSocket disconnect rate |
| ASR WebSocket uptime | >99.9% | High | Deepgram connection status |
| Fireworks API availability | >99.9% | Critical | Ocean-1 inference success |
| PostgreSQL write latency | <50ms | Medium | apiserver upsert timing |
| Redis pub/sub latency | <20ms | Medium | Event delivery timing |
| Shard autoscaling lag | <60s | Medium | K8s pod startup time |

---

## Bottleneck Identification

### Known Bottlenecks (From Architecture)

1. **ASR Processing**: <300ms latency dominates budget but already optimized âœ…
2. **Ocean-1 Inference**: Variable latency, external dependency âš ï¸
3. **Batching Window**: 100-300ms added when triggered âš ï¸
4. **Network Jitter**: Variable based on regional proximity

### Optimization Opportunities

1. **Regional Colocation**: Deploy Cresta in same AWS region as Connect
2. **Warm Shard Pool**: Pre-scale shards before peak traffic
3. **Adaptive Batching**: Dynamically adjust window based on latency SLA
4. **Edge Caching**: Cache frequent Ocean-1 responses (e.g., common FAQs)

---

## Failure Mode Flows

### Scenario 1: Deepgram ASR Unavailable

```
gowalter ASR Client
   â†“ (WebSocket connection fails)
Recovery Manager Activates
   â”œâ”€ Retry with exponential backoff
   â””â”€ If recovery fails after N attempts:
      â””â”€ ðŸŸ¡ Unknown: Failover to backup ASR or graceful degradation?
         â””â”€ **Requires vendor clarification**
```

**Impact**: No transcription = no ML guidance
**Severity**: Critical

---

### Scenario 2: Fireworks AI Unavailable

```
Model Shard (Ocean-1 + LoRA)
   â†“ (Inference request to Fireworks)
Fireworks API Timeout/Error
   â””â”€ ðŸŸ¡ Unknown: Local fallback or error to orchestrator?
      â”œâ”€ Option A: Return error, skip Ocean-1 annotations
      â”œâ”€ Option B: Fallback to simpler model (e.g., GPT-3.5)
      â””â”€ **Requires vendor clarification**
```

**Impact**: No Ocean-1 guidance (RAG, summarization)
**Severity**: High (but other classifiers may continue)

---

### Scenario 3: PostgreSQL Write Failure

```
apiserver Transcript Upsert
   â†“ (Database write fails)
Error Handling
   â”œâ”€ Retry with backoff
   â”œâ”€ Log to error tracking (Sentry, CloudWatch)
   â””â”€ Continue event publishing to Redis
      â””â”€ ML processing continues (uses in-memory transcript)
      â””â”€ Transcript persistence delayed but eventually consistent
```

**Impact**: Temporary transcript loss, but real-time ML continues
**Severity**: Medium

---

## Data Sovereignty & Regional Routing

### Verified Regional Architecture
**Source**: Cresta documentation on customer subdomains

```
EU Customer Call
   â†“
Amazon Connect (EU region)
   â†“ (Regional KVS)
customer.eu.cresta.ai
   â”œâ”€ Cresta infrastructure deployed in EU AWS region
   â”œâ”€ Data stays within EU for GDPR compliance
   â””â”€ Regional Deepgram ASR endpoint (ðŸŸ¡ needs confirmation)

Fireworks AI Inference
   â””â”€ ðŸŸ¡ Critical Question: Are Fireworks endpoints regional?
      â”œâ”€ If no: Data transits to US (potential GDPR issue)
      â””â”€ If yes: Confirm regional deployment matches Cresta regions
```

**Follow-Up Required**:
- Fireworks AI regional deployment map
- Data Processing Agreement (DPA) coverage for cross-border ML inference
- Customer ability to specify inference region

---

**Document Status**: Comprehensive data flow sequences based on verified architecture. Critical latency dependencies and failure modes identified. Specific benchmarks and failure handling require vendor clarification.
